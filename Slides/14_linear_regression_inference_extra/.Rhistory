X <- ames_train %>%
mutate(int = 1) %>%
select(int, living_area = total_living_area, quality_score, year_built, bedrooms) %>%
as.matrix()
X
Y <- ames_train[, 1]
Y
n <- length(Y)
beta <- solve(t(X) %*% X) %*% t(X) %*% Y
solve(t(X) %*% X)
solve(t(X) %*% X) %*% t(X)
solve(t(X) %*% X) %*% t(X) %*% Y
Y
Y <- ames_train[, 1] %>%
as.matrix
Y
beta <- solve(t(X) %*% X) %*% t(X) %*% Y
beta
# compute predictions
yhat <- X %*% beta
yhat
# compute predictions
Yhat <- X %*% beta
Yhat
# compute residuals
r <- Y - Yhat
r
dim(X)
sum(r^2) / 5
(Y - Yhat)^T %*% (Y - Yhat) / 5
(Y - Yhat)
(Y - Yhat)^T
Y <- ames_train[, 1] %>%
as.matrix
Y
n <- length(Y)
beta <- solve(t(X) %*% X) %*% t(X) %*% Y
beta
# compute predictions
Yhat <- X %*% beta
Yhat
# compute residuals
r <- Y - Yhat
r
sum(r^2) / 5
(Y - Yhat)^T %*% (Y - Yhat) / 5
(Y - Yhat)
(Y - Yhat)^T
Y
class(Y)
class(Yhat)
class(Y - Yhat )
Y - Yhat
(Y - Yhat)^T
(as.matrix(Y - Yhat))^T
t(Y - Yhat) %*% (Y - Yhat) / 5
sum(r^2) / 5
t(X) %*% X
solve(t(X) %*% X)
968,972,338 * 0.1667
968972338 * 0.1667
sqrt(968972338 * 0.1667)
sigma2 <- sum(r^2) / 5
sigma2
sigma2 <- t(Y - Yhat) %*% (Y - Yhat) / 5
sigma2
p <- ncol(X)
p
beta
XtX_inv <- solve(t(X) %*% X)
beta <- XtX_inv %*% t(X) %*% Y
beta
sigma2_beta <- sigma2 * diag(XtX_inv)
sigma2_beta
sigma2
diag(XtX_inv)
# compute the SD of beta_hat
sigma2_beta <- sigma2 * diag(XtX_inv)
sigma2_beta
# two ways of computing sigma2
sigma2 <- t(Y - Yhat) %*% (Y - Yhat) / (n-p)
sigma2
sigma2 <- sum(r^2) / (n-p)
sigma2
# compute the SD of beta_hat
sigma2_beta <- sigma2 * diag(XtX_inv)
sigma2_beta
# standardized coefficients
t <- beta / sigma2_beta
t
# compute p-value for each beta0
p <- 2*(1 - pt(abs(t), n-p))
p
# note that these values match what we get from summary(lm())
ls_fit <- lm(sale_price ~ ., ames_train)
summary(ls_fit)
beta
sigma2_beta
# standardized coefficients
t <- beta / sqrt{sigma2_beta}
t
# standardized coefficients
t <- beta / sqrt(sigma2_beta)
t
# compute p-value for each beta0
p <- 2*(1 - pt(abs(t), n-p))
p
# note that these values match what we get from summary(lm())
ls_fit <- lm(sale_price ~ ., ames_train)
summary(ls_fit)
sigma2_beta
sqrt(sigma2_beta)
n-p
library(tidyverse)
library(scales)
ames_train <- read_csv("ames_train_multi.csv") %>%
select(-neighborhood)
ames_train %>% print(width = Inf)
# For the toy example in the slides, we use only the first 10 rows
ames_train <- ames_train[1:10, ]
# Get
X <- ames_train %>%
mutate(int = 1) %>%
select(int, living_area = total_living_area, quality_score, year_built, bedrooms) %>%
as.matrix()
X
Y <- ames_train[, 1] %>%
as.matrix
Y
n <- length(Y)
p <- ncol(X)
n-p
XtX_inv <- solve(t(X) %*% X)
beta <- XtX_inv %*% t(X) %*% Y
beta
# compute predictions
Yhat <- X %*% beta
Yhat
# compute residuals
r <- Y - Yhat
r
# two ways of computing sigma2
sigma2 <- t(Y - Yhat) %*% (Y - Yhat) / (n-p)
sigma2
sigma2 <- sum(r^2) / (n-p)
sigma2
# compute the SD of beta_hat
sigma2_beta <- sigma2 * diag(XtX_inv)
sigma2_beta
# standardized coefficients
t <- beta / sqrt(sigma2_beta)
t
# compute p-value for each beta0
p <- 2*(1 - pt(abs(t), n-p))
p
# note that these values match what we get from summary(lm())
ls_fit <- lm(sale_price ~ ., ames_train)
summary(ls_fit)
100 / 15
qchisq(0.05, 2)
dchisq(0.05, 2)
1 - pchisq(0.5, 2)
1 - pchisq(0.1, 2)
dchisq(0.95, 2)
1 - pchisq(0.3, 2)
dchisq(0.05, 2)
qchisq(0.95, 2)
1 - pchisq(6, 2)
14 + 17 + 8 + 5
data <- data.frame(i = 1:10,
x = c(1, -0.5, 1.3, -0.2, 0.2, 1, 1.8, -1.1, 0.6, -0.8),
y = c(0.4, -0.4, 0.6, 0.2, 0.3, 0.8, 0.8, -0.5, 0.5, -0.2))
# note that these values match what we get from summary(lm())
ls_fit <- lm(sale_price ~ total_living_area, ames_train)
summary(ls_fit)
library(tidyverse)
library(scales)
ames_train <- read_csv("ames_train.csv")
ames_train %>% print(width = Inf)
# For the toy example in the slides, we use only the first 10 rows
#ames_train <- ames_train[1:10, ]
x <- ames_train$total_living_area
x
y <- ames_train$sale_price
y
n <- length(y)
# estimate coefficients using LS
b1 <- cov(x, y) / var(x)
b1
# alternatively
b1 <- cor(x, y) * sd(y) / sd(x)
b1
b0 <- mean(y) - b1 * mean(x)
b0
# compute predictions
yhat <- b0 + b1 * x
# compute residuals
r <- y - yhat
# estimate sigma2
sigma2_est <- sum(r^2) / (n-2)
# compute SD of beta0
sigma_b0_est <- sqrt(sigma2_est * sum(x^2) / (n * sum(x^2) - sum(x)^2))
sigma_b0_est
# compute SD of beta1
sigma_b1_est <- sqrt(n * sigma2_est / (n * sum(x^2) - sum(x)^2))
sigma_b1_est
# compute test statistic of beta0
t_b0 <- b0 / sigma_b0_est
t_b0
# compute test statistic of beta1
t_b1 <- b1 / sigma_b1_est
t_b1
# compute p-value for beta0
p_b0 <- 2*(1 - pt(abs(t_b0), n-2))
p_b0
# compute p-value for beta1
p_b1 <- 2*(1 - pt(abs(t_b1), n-2))
p_b1
# note that these values match what we get from summary(lm())
ls_fit <- lm(sale_price ~ total_living_area, ames_train)
summary(ls_fit)
library(tidyverse)
library(scales)
ames_train <- read_csv("ames_train.csv")
ames_train %>% print(width = Inf)
# For the toy example in the slides, we use only the first 10 rows
ames_train <- ames_train[1:10, ]
x <- ames_train$total_living_area
x
y <- ames_train$sale_price
y
n <- length(y)
# estimate coefficients using LS
b1 <- cov(x, y) / var(x)
b1
# alternatively
b1 <- cor(x, y) * sd(y) / sd(x)
b1
b0 <- mean(y) - b1 * mean(x)
b0
# compute predictions
yhat <- b0 + b1 * x
# compute residuals
r <- y - yhat
# estimate sigma2
sigma2_est <- sum(r^2) / (n-2)
# compute SD of beta0
sigma_b0_est <- sqrt(sigma2_est * sum(x^2) / (n * sum(x^2) - sum(x)^2))
sigma_b0_est
# compute SD of beta1
sigma_b1_est <- sqrt(n * sigma2_est / (n * sum(x^2) - sum(x)^2))
sigma_b1_est
# compute test statistic of beta0
t_b0 <- b0 / sigma_b0_est
t_b0
# compute test statistic of beta1
t_b1 <- b1 / sigma_b1_est
t_b1
# compute p-value for beta0
p_b0 <- 2*(1 - pt(abs(t_b0), n-2))
p_b0
# compute p-value for beta1
p_b1 <- 2*(1 - pt(abs(t_b1), n-2))
p_b1
# note that these values match what we get from summary(lm())
ls_fit <- lm(sale_price ~ total_living_area, ames_train)
summary(ls_fit)
library(tidyverse)
ames_train <- read_csv("ames_train.csv")
ames_train %>% print(width = Inf)
x <- ames_train$total_living_area
x
ames_train <- ames_train[1:10, ]
ames_train
x <- ames_train$total_living_area
x
y <- ames_train$sale_price
y
n <- length(y)
# estimate coefficients using LS
b1 <- cov(x, y) / var(x)
b1
# alternatively
b1 <- cor(x, y) * sd(y) / sd(x)
b1
b0 <- mean(y) - b1 * mean(x)
b0
# compute predictions
yhat <- b0 + b1 * x
yhat
# compute residuals
r <- y - yhat
r
r^2
sum(r^2)
# estimate sigma2
sigma2_est <- sum(r^2) / (n-2)
sigma2_est
# compute SD of beta0
sigma_b0_est <- sqrt(sigma2_est * sum(x^2) / (n * sum(x^2) - sum(x)^2))
sigma_b0_est
# compute SD of beta1
sigma_b1_est <- sqrt(n * sigma2_est / (n * sum(x^2) - sum(x)^2))
sigma_b1_est
# compute test statistic of beta0
t_b0 <- b0 / sigma_b0_est
t_b0
# compute test statistic of beta1
t_b1 <- b1 / sigma_b1_est
t_b1
# compute p-value for beta0
p_b0 <- 2*(1 - pt(abs(t_b0), n-2))
p_b0
# compute p-value for beta1
p_b1 <- 2*(1 - pt(abs(t_b1), n-2))
p_b1
# note that these values match what we get from summary(lm())
ls_fit <- lm(sale_price ~ total_living_area, ames_train)
ls_fit
summary(ls_fit)
summary(ls_fit)
r
yhat
data.frame(residuals = r, yhat = yhat)
data.frame(residuals = r, yhat = yhat) %>%
ggplot() +
geom_point(aes(x = yhat, y = residuals),
alpha = 0.5)
data.frame(residuals = r, yhat = yhat) %>%
ggplot() +
geom_point(aes(x = yhat, y = residuals),
alpha = 0.5) +
geom_hline(yintercept = 0)
data.frame(residuals = r, yhat = yhat) %>%
ggplot() +
geom_point(aes(x = yhat, y = residuals),
alpha = 0.5) +
geom_hline(yintercept = 0) +
theme_classic()
library(scales)
data.frame(residuals = r, yhat = yhat) %>%
ggplot() +
geom_point(aes(x = yhat, y = residuals),
alpha = 0.5) +
geom_hline(yintercept = 0) +
theme_classic() +
scale_x_continuous(breaks = 1:5 * 1e05,
labels = comma(1:5 * 1e05)) +
scale_y_continuous(breaks = -2:3 * 1e05,
labels = comma(-2:3 * 1e05))
data.frame(residuals = r, yhat = yhat) %>%
ggplot() +
geom_point(aes(x = yhat, y = residuals),
alpha = 0.5) +
geom_hline(yintercept = 0) +
theme_classic() +
scale_x_continuous(breaks = 1:5 * 1e05,
labels = comma(1:5 * 1e05))
# recreate residual plot for multiple regression
ames_train_multi <- read_csv("ames_train_multi.csv")
ls_fit_multi <- lm(sale_price ~ total_living_area + quality_score +
year_built + bedrooms, ames_train_multi)
summary(ls_fit_multi)
# recreate residual plot for multiple regression
ames_train_multi <- read_csv("ames_train_multi.csv")
head(ames_train_multi)
ls_fit_multi <- lm(sale_price ~ total_living_area + quality_score +
year_built + bedrooms, ames_train_multi)
summary(ls_fit_multi)
ls_fit_multi$residuals
names(ls_fit_multi)
ls_fit_multi$coefficients
data.frame(residuals = ls_fit_multi$residuals,
yhat = ls_fit_multi$fitted) %>%
ggplot() +
geom_point(aes(x = yhat, y = residuals),
alpha = 0.5) +
geom_hline(yintercept = 0) +
theme_classic() +
scale_x_continuous(breaks = 1:5 * 1e05,
labels = comma(1:5 * 1e05)) +
scale_y_continuous(breaks = -2:3 * 1e05,
labels = comma(-2:3 * 1e05))
ames_train %>%
select_if(is.numeric) %>%
pivot_longer(cols = everything()) %>%
ggplot() +
geom_histogram(aes(x = value)) +
facet_wrap(~name, scales = "free")
# Fitting LS and LAD lines
library(tidyverse)
library(scales)
ames_train <- read_csv("ames_train_multi.csv")
ames_train %>% print(width = Inf)
ames_train %>%
select_if(is.numeric) %>%
pivot_longer(cols = everything()) %>%
ggplot() +
geom_histogram(aes(x = value)) +
facet_wrap(~name, scales = "free")
# let's log-transform sale price, total_living area and year built
ames_train %>%
transmute(bedrooms = bedrooms,
quality_score = quality_score,
# log-transform these three variables:
log_sale_price = log(sale_price),
log_total_living_area = log(total_living_area),
log_year_built = log(year_built)) %>%
select_if(is.numeric) %>%
pivot_longer(cols = everything()) %>%
mutate(name = fct_inorder(name)) %>%
ggplot() +
geom_histogram(aes(x = value)) +
facet_wrap(~name, scales = "free")
# look at the relationship between sale price and other features
gg_beds_price <- ames_train %>%
ggplot() +
geom_boxplot(aes(x = bedrooms, y = sale_price, group = bedrooms))
gg_year_price <- ames_train %>%
ggplot() +
geom_point(aes(x = year_built, y = sale_price),
alpha = 0.4)
gg_quality_price <- ames_train %>%
ggplot() +
geom_boxplot(aes(x = quality_score, y = sale_price, group = quality_score))
gg_area_price <- ames_train %>%
ggplot() +
geom_point(aes(x = total_living_area, y = sale_price),
alpha = 0.4)
library(patchwork)
(gg_beds_price + gg_year_price) / (gg_quality_price + gg_area_price)
# look at the relationship between log(sale price) and other features
gg_beds_log_price <- ames_train %>%
ggplot() +
geom_boxplot(aes(x = bedrooms, y = log(sale_price), group = bedrooms))
gg_year_log_price <- ames_train %>%
ggplot() +
geom_point(aes(x = year_built, y = log(sale_price)),
alpha = 0.4)
gg_quality_log_price <- ames_train %>%
ggplot() +
geom_boxplot(aes(x = quality_score, y = log(sale_price), group = quality_score))
gg_area_log_price <- ames_train %>%
ggplot() +
geom_point(aes(x = total_living_area, y = log(sale_price)),
alpha = 0.4)
(gg_beds_log_price + gg_year_log_price) / (gg_quality_log_price + gg_area_log_price)
# untransformed fit
lm1 <- lm(sale_price ~ .,
data = ames_train)
pred1 <- predict(lm1, ames_test)
#rsq
cor(pred1, ames_test$sale_price)^2
# log-transformed response fit
lm2 <- lm(log(sale_price) ~ .,
data = ames_train)
pred2 <- exp(predict(lm2, ames_test)) # note the exponentiation
#rsq
cor(pred2, ames_test$sale_price)^2
# log-transformed predictors fit
lm3 <- lm(sale_price ~ log(total_living_area) + neighborhood +
year_built + bedrooms + quality_score,
data = ames_train)
pred3 <- predict(lm3, ames_test)
#rsq
cor(pred3, ames_test$sale_price)^2
# log-transformed response and predictors
lm4 <- lm(log(sale_price) ~ log(total_living_area) + neighborhood +
year_built + bedrooms + quality_score,
data = ames_train)
pred4 <- exp(predict(lm4, ames_test))
#rsq
cor(pred4, ames_test$sale_price)^2
# no transformation
lm_untransformed <- lm(sale_price ~ quality_score, ames_train)
summary(lm_untransformed)$r.squared
# plot the fit
ggplot(ames_train) +
geom_boxplot(aes(x = as.factor(quality_score),
y = sale_price)) +
geom_abline(intercept = lm_untransformed$coef[1],
slope = lm_untransformed$coef[2]) +
theme_classic() +
scale_y_continuous("Sale Price", breaks = seq(0, 6, 2) * 100000,
labels = scales::dollar(seq(0, 6, 2) * 100000)) +
scale_x_discrete("Quality score")
# plot the fit
ggplot(ames_train) +
geom_boxplot(aes(x = as.factor(quality_score),
y = sale_price)) +
geom_line(aes(x = x, y = y),
data = fit_log_resp) +
theme_classic() +
scale_y_continuous("Sale Price", breaks = seq(0, 6, 2) * 100000,
labels = scales::dollar(seq(0, 6, 2) * 100000)) +
scale_x_discrete("Quality score")
# Log transformation of response
lm_log_resp <- lm(log(sale_price) ~ quality_score, ames_train)
summary(lm_log_resp)$r.squared
fit_log_resp <- data.frame(x = 1:10,
y = exp(predict(lm_log_resp,
newdata = data.frame(quality_score = 1:10))))
# plot the fit
ggplot(ames_train) +
geom_boxplot(aes(x = as.factor(quality_score),
y = sale_price)) +
geom_line(aes(x = x, y = y),
data = fit_log_resp) +
theme_classic() +
scale_y_continuous("Sale Price", breaks = seq(0, 6, 2) * 100000,
labels = scales::dollar(seq(0, 6, 2) * 100000)) +
scale_x_discrete("Quality score")
